dataset: "gsm8k"
data_dir: "./data"
output_dir: "./experiments/gpt2-gsm8k"
lora_rank: 32
lr: 5e-6
beta: 0.005
weight_decay: 0.1
warmup_ratio: 0.1
lr_scheduler_type: "cosine"
optimizer: "paged_adamw_8bit"
max_grad_norm: 0.1
group_size: 8
temperature: 0.5
gradient_accumulation_steps: 4
per_device_train_batch_size: 8
max_prompt_length: 1024
max_completion_length: 1024
num_train_epochs: 1
save_steps: 250
save_total_limit: 3
lambda_penalty: 0.05
gate_penalty: 0.1
gate_temperature: 1.0